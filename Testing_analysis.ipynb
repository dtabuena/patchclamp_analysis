{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyPw1Z3H+I2Ot4Lk/rYfiMPD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dtabuena/patchclamp_analysis/blob/main/Testing_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall patchclamp_analysis -y\n",
        "!pip install https://github.com/dtabuena/patchclamp_analysis/archive/main.zip -q --upgrade\n",
        "from patchclamp_analysis import (\n",
        "    ephys_utilities,\n",
        "    abf_handling,\n",
        "    analysis_parameters,\n",
        "    capacitance_analysis,\n",
        "    firing_gain_analysis,\n",
        "    input_resistance_analyzer,\n",
        "    iv_analysis,\n",
        "    resting_potential_analysis,\n",
        "    rheobase_analaysis,\n",
        "    spike_latency_analysis,\n",
        "    pipeline_functions\n",
        ")"
      ],
      "metadata": {
        "id": "XwpjYpjOpHSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from patchclamp_analysis.pipeline_functions import (analysis_iterator_h5,\n",
        "                                                    build_analysis_h5,\n",
        "                                                    print_h5_tree,\n",
        "                                                    restratify_h5_by_attribute,\n",
        "                                                    add_current_density_datasets,\n",
        "                                                    get_datasets_from_h5,\n",
        "                                                    flatten_nested_column,\n",
        "                                                    write_dict_to_excel)"
      ],
      "metadata": {
        "id": "_Pn4LJ1tLCvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "V3_QEt4DLnv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "isxgnmFpprlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from IPython.display import clear_output\n",
        "import matplotlib as mpl\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pyabf\n",
        "import h5py\n",
        "import urllib\n",
        "\n",
        "working_dir = r'\\\\hive.gladstone.internal\\Huang-Lab\\Lab Members\\Tabuena_Main\\0_Projects_hv\\testing_h5'\n",
        "os.chdir(working_dir)\n",
        "\n",
        "response = urllib.request.urlretrieve('https://raw.githubusercontent.com/dtabuena/Resources/main/Matplotlib_Config/Load_FS6.py','Load_FS6.py')\n",
        "%run Load_FS6.py"
      ],
      "metadata": {
        "id": "p0nMzKwVe26h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset_info = {'data_name': 'MAPT_DLX_SST',\n",
        "           'data_source': os.path.join(working_dir),\n",
        "           'file_naming_scheme': ['rec_date','genotype','sex','age', 'orientation','slice','cell_num','cell_type'],\n",
        "           }\n",
        "\n",
        "\n",
        "##### Setup Protocol List\n",
        "VC_prot = ['VC - MemTest-10ms-160ms',\n",
        "           'VC - Multi IV - 150ms',\n",
        "           'VC - Multi IV - 500ms']\n",
        "IC_prot = ['IC - Gain - D10pA',\n",
        "           'IC - Gain - D20pA',\n",
        "           'IC - Gain - D50pA',\n",
        "           'IC - Rheobase',\n",
        "           'IC - R input',\n",
        "           'IC - Latentcy 800pA-1s'\n",
        "           'VC - 3min GapFree',\n",
        "           'I0 - 3min GapFree']\n",
        "\n",
        "analysis_dir =os.path.join(working_dir,dataset_info['data_source'])\n",
        "os.makedirs(analysis_dir,exist_ok=True)\n",
        "os.chdir(analysis_dir)\n",
        "print(os.getcwd())\n"
      ],
      "metadata": {
        "id": "8fNANEJjbXvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_h5_tree(filepath, head=None, give_tree=False):\n",
        "    def recurse(obj, prefix=\"\"):\n",
        "        lines = []\n",
        "        keys = list(obj.keys())\n",
        "        has_attrs = len(obj.attrs) > 0\n",
        "\n",
        "        # Print attributes first if they exist\n",
        "        if has_attrs:\n",
        "            lines.append(f\"{prefix}|-- [ATTRS]: {len(obj.attrs)} attributes\")\n",
        "\n",
        "        for i, key in enumerate(keys):\n",
        "            item = obj[key]\n",
        "            is_last = (i == len(keys) - 1)\n",
        "            connector = \"`-- \" if is_last else \"|-- \"\n",
        "\n",
        "            if isinstance(item, h5py.Group):\n",
        "                item_has_attrs = len(item.attrs) > 0\n",
        "                item_has_datasets = len(item.keys()) > 0\n",
        "\n",
        "                if not item_has_datasets and not item_has_attrs:\n",
        "                    lines.append(f\"{prefix}{connector}[G] {key}/ (empty)\")\n",
        "                else:\n",
        "                    lines.append(f\"{prefix}{connector}[G] {key}/\")\n",
        "                    extension = \"    \" if is_last else \"|   \"\n",
        "                    lines.extend(recurse(item, prefix + extension))\n",
        "            else:\n",
        "                lines.append(f\"{prefix}{connector}[D] {key}: {item.shape} {item.dtype}\")\n",
        "\n",
        "        return lines\n",
        "\n",
        "    with h5py.File(filepath, 'r') as f:\n",
        "        tree_lines = [f.filename]\n",
        "        tree_lines.extend(recurse(f))\n",
        "\n",
        "    full_tree = '\\n'.join(tree_lines)\n",
        "\n",
        "    # Print only the specified number of lines\n",
        "    if head is not None:\n",
        "        lines_to_print = tree_lines[:head]\n",
        "        print('\\n'.join(lines_to_print))\n",
        "        if len(tree_lines) > head:\n",
        "            print(f\"\\n... ({len(tree_lines) - head} more lines)\")\n",
        "    else:\n",
        "        print(full_tree)\n",
        "\n",
        "    if give_tree:\n",
        "        return full_tree\n",
        "    else:\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "mD8YLWU8gRzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_analysis_h5(dataset_info):\n",
        "    data_name = dataset_info['data_name']\n",
        "    data_source = dataset_info['data_source']\n",
        "    file_naming_scheme = dataset_info['file_naming_scheme']\n",
        "\n",
        "    h5_filename = f'{data_name}_analysis_recs.h5'\n",
        "\n",
        "    file_list = [os.path.join(dirpath, filename)\n",
        "             for dirpath, dirnames, filenames in os.walk(data_source)\n",
        "             for filename in filenames\n",
        "             if filename.endswith('.abf')]\n",
        "\n",
        "    with h5py.File(h5_filename, 'w') as hf:\n",
        "        single_files_group = hf.create_group('abf_files')\n",
        "\n",
        "        for file_id, file_name in enumerate(file_list):\n",
        "            file_metadata = dict()\n",
        "            base_name = os.path.basename(file_name)\n",
        "            file_metadata['recording_name'] = base_name\n",
        "            parsed_name = parse_name(base_name, file_naming_scheme)\n",
        "            file_metadata.update(parsed_name)\n",
        "\n",
        "            abf = pyabf.ABF(file_name)\n",
        "            file_metadata['protocol'] = abf.protocol\n",
        "            file_metadata['channelList'] = str(abf.channelList)\n",
        "            file_metadata['abf_timestamp'] = str(abf.abfDateTime)\n",
        "\n",
        "            # Create group for this recording\n",
        "            rec_group = single_files_group.create_group(base_name)\n",
        "\n",
        "            # Add ALL metadata as attributes to the group\n",
        "            rec_group.attrs['file_id'] = file_id\n",
        "            rec_group.attrs['filepath'] = str(file_name)\n",
        "            for key, value in file_metadata.items():\n",
        "                rec_group.attrs[key] = value\n",
        "\n",
        "    print(f\"Saved metadata for {len(file_list)} files to {h5_filename}\")\n",
        "    return h5_filename\n",
        "\n",
        "def parse_name(base_name,scheme):\n",
        "    parsed_name=dict()\n",
        "    split_words = base_name.split('_')\n",
        "    re_code = ['_'+split_words[i] for i in range(len(scheme))]\n",
        "    re_code = ''.join(re_code)[1:]\n",
        "    parsed_name['cell_id']= re_code\n",
        "    for ci in range(len(scheme)):\n",
        "        parsed_name[scheme[ci]] = split_words[ci]\n",
        "    return parsed_name\n",
        "\n"
      ],
      "metadata": {
        "id": "KnOHFzaggR1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Import And Catalog ABFS\n",
        "\"\"\"\n",
        "\n",
        "h5_data_loc = build_analysis_h5(dataset_info)\n",
        "print_h5_tree(h5_data_loc, head=10)"
      ],
      "metadata": {
        "id": "CVmHkAYCyqgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analysis_iterator_h5(h5_data_loc, analyzer_configs):\n",
        "    \"\"\"\n",
        "    Loop through recording groups in single_files, run analysis, save results to groups.\n",
        "\n",
        "    Parameters:\n",
        "    - h5_data_loc: path to HDF5 file\n",
        "    - analyzer_configs: dict mapping protocol names to analyzer functions\n",
        "                        e.g., {'IV': {'func': analyze_iv, 'arg1': val1}, ...}\n",
        "    \"\"\"\n",
        "\n",
        "    with h5py.File(h5_data_loc, 'a') as f:\n",
        "        abf_files = f['abf_files']\n",
        "\n",
        "        for base_name in tqdm(list(abf_files.keys()), desc=\"Processing recordings\"):\n",
        "            grp = abf_files[base_name]\n",
        "\n",
        "            # Get metadata from attributes\n",
        "            filepath = grp.attrs['filepath']\n",
        "            protocol = grp.attrs['protocol']\n",
        "\n",
        "            print(f\"\\nProcessing: {base_name}\")\n",
        "\n",
        "            # Get appropriate analyzer and run\n",
        "            if protocol in analyzer_configs:\n",
        "                config = analyzer_configs[protocol].copy()\n",
        "                analyzer_func = config.pop('func')\n",
        "                abf = pyabf.ABF(filepath)\n",
        "                results = analyzer_func(abf, **config)\n",
        "            else:\n",
        "                print(f\"  No analyzer for protocol: {protocol}\")\n",
        "                results = {}\n",
        "\n",
        "            # Save results to the group\n",
        "            for key, val in results.items():\n",
        "                # Delete if exists (overwrite)\n",
        "                if key in grp:\n",
        "                    del grp[key]\n",
        "\n",
        "                if isinstance(val, dict):\n",
        "                    # Nested dict -> dataset (structured array)\n",
        "                    keys_arr = np.array(list(val.keys()))\n",
        "                    vals_arr = np.array(list(val.values()))\n",
        "                    dt = np.dtype([('key', keys_arr.dtype), ('value', vals_arr.dtype)])\n",
        "                    data = np.array(list(zip(keys_arr, vals_arr)), dtype=dt)\n",
        "                    grp.create_dataset(key, data=data)\n",
        "                elif val is not None:\n",
        "                    # Scalar or array -> dataset\n",
        "                    grp.create_dataset(key, data=val)\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "-xJN1Eon_5vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Analyze All Recordings\n",
        "\"\"\"\n",
        "os.chdir(working_dir)\n",
        "analyzer_configs = analysis_parameters.init_func_arg_dicts_h5()\n",
        "analysis_iterator_h5(h5_data_loc,analyzer_configs)"
      ],
      "metadata": {
        "id": "liyhGO3inhTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "START HERE TO SKIP ANALYSIS\n",
        "\"\"\"\n",
        "h5_data_loc = 'MAPT_DLX_SST_analysis_recs.h5'"
      ],
      "metadata": {
        "id": "BdKryQAygRwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "\n",
        "def restratify_h5_by_attribute(input_file, output_file, grouping_attr):\n",
        "    \"\"\"\n",
        "    Reorganize HDF5 file grouping by a specific attribute\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    input_file : str\n",
        "        Path to input HDF5 file\n",
        "    output_file : str\n",
        "        Path to output HDF5 file\n",
        "    grouping_attr : str\n",
        "        Attribute name to group by (e.g., 'cell_id')\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    str\n",
        "        Path to output file\n",
        "    \"\"\"\n",
        "    with h5py.File(input_file, 'r') as f_in, h5py.File(output_file, 'w') as f_out:\n",
        "\n",
        "        # Check if 'abf_files' group exists\n",
        "        if 'abf_files' not in f_in:\n",
        "            print(\"Error: 'abf_files' group not found in input file\")\n",
        "            return output_file\n",
        "\n",
        "        abf_files = f_in['abf_files']\n",
        "\n",
        "        # Collect groups by the specified attribute\n",
        "        attr_map = defaultdict(list)\n",
        "\n",
        "        for group_name in abf_files.keys():\n",
        "            group = abf_files[group_name]\n",
        "            if isinstance(group, h5py.Group) and grouping_attr in group.attrs:\n",
        "                attr_value = group.attrs[grouping_attr]\n",
        "                attr_map[attr_value].append((group_name, group))\n",
        "\n",
        "        # Create new structure\n",
        "        for attr_value, recordings in attr_map.items():\n",
        "            # Use attribute value directly as group name\n",
        "            new_group = f_out.create_group(str(attr_value))\n",
        "            new_group.attrs[grouping_attr] = attr_value\n",
        "\n",
        "            # Copy shared metadata from first recording\n",
        "            first_rec = recordings[0][1]\n",
        "            for key, value in first_rec.attrs.items():\n",
        "                if key not in new_group.attrs:\n",
        "                    new_group.attrs[key] = value\n",
        "\n",
        "            # Merge all datasets from all recordings directly into the cell group\n",
        "            for orig_name, orig_group in recordings:\n",
        "                # Copy all datasets directly\n",
        "                for dataset_name in orig_group.keys():\n",
        "                    if isinstance(orig_group[dataset_name], h5py.Dataset):\n",
        "                        target_name = dataset_name\n",
        "                        counter = 1\n",
        "                        while target_name in new_group:\n",
        "                            target_name = f'{dataset_name}_{counter}'\n",
        "                            counter += 1\n",
        "                        f_in.copy(f'abf_files/{orig_name}/{dataset_name}', new_group, name=target_name)\n",
        "\n",
        "        print(f\"Restratified by '{grouping_attr}'\")\n",
        "        print(f\"Created {len(attr_map)} groups\")\n",
        "        print(f\"Total source recordings merged: {sum(len(recs) for recs in attr_map.values())}\")\n",
        "        print(f\"Output: {output_file}\")\n",
        "\n",
        "    return output_file\n",
        "\n",
        "# Usage:\n",
        "cell_h5_loc = restratify_h5_by_attribute(h5_data_loc, 'MAPT_DLX_SST_analysis_cells.h5', 'cell_id')"
      ],
      "metadata": {
        "id": "lDuzngAAgRuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_h5_tree(cell_h5_loc,head=20)"
      ],
      "metadata": {
        "id": "QCwFBx0CFqtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_current_density_datasets(h5_filename, capacitance_key='Cmq_160.0', iv_datasets=['IV_K_130_140', 'IV_Na_0.2_10']):\n",
        "    \"\"\"\n",
        "    Add current density datasets to HDF5 file groups.\n",
        "\n",
        "    For each file group:\n",
        "    1. Get capacitance value from dataset\n",
        "    2. For each IV dataset, create a density version\n",
        "    3. Divide current by capacitance (or fill with NaN if invalid)\n",
        "    4. Save as new dataset with 'density_' prefix\n",
        "\n",
        "    Parameters:\n",
        "    - h5_filename: path to HDF5 file\n",
        "    - capacitance_key: dataset name for capacitance (default 'Cmq_160.0')\n",
        "    - iv_datasets: list of IV dataset names to convert\n",
        "\n",
        "    Returns:\n",
        "    - list of groups that failed\n",
        "    \"\"\"\n",
        "    failed_groups = []\n",
        "    total_groups = 0\n",
        "    successful_groups = 0\n",
        "\n",
        "    with h5py.File(h5_filename, 'a') as f:\n",
        "        # Loop through all groups at the top level\n",
        "        for group_name in f.keys():\n",
        "            grp = f[group_name]\n",
        "\n",
        "            # Skip if not a group\n",
        "            if not isinstance(grp, h5py.Group):\n",
        "                continue\n",
        "\n",
        "            total_groups += 1\n",
        "\n",
        "            # Get capacitance value from dataset (not attribute)\n",
        "            capacitance_valid = False\n",
        "            error_msg = None\n",
        "\n",
        "            if capacitance_key not in grp:\n",
        "                error_msg = f'{capacitance_key} not found'\n",
        "            else:\n",
        "                capacitance = grp[capacitance_key][()]\n",
        "                if pd.isna(capacitance):\n",
        "                    error_msg = f'{capacitance_key} is NaN'\n",
        "                elif capacitance <= 0:\n",
        "                    error_msg = f'{capacitance_key} is {capacitance} (invalid)'\n",
        "                else:\n",
        "                    capacitance_valid = True\n",
        "\n",
        "            # Process each IV dataset\n",
        "            group_success = False\n",
        "            for iv_name in iv_datasets:\n",
        "                if iv_name not in grp:\n",
        "                    continue\n",
        "\n",
        "                density_name = f'density_{iv_name}'\n",
        "                if density_name in grp:\n",
        "                    del grp[density_name]\n",
        "\n",
        "                # Read original IV data (voltage, current) pairs\n",
        "                iv_data = grp[iv_name][:]\n",
        "\n",
        "                # Create new array\n",
        "                density_data = np.zeros_like(iv_data)\n",
        "                density_data['key'] = iv_data['key']  # Voltage stays same\n",
        "\n",
        "                if capacitance_valid:\n",
        "                    # Divide current by capacitance\n",
        "                    density_data['value'] = iv_data['value'] / capacitance\n",
        "                    group_success = True\n",
        "                else:\n",
        "                    # Fill with NaN\n",
        "                    density_data['value'] = np.full(len(iv_data), np.nan)\n",
        "\n",
        "                grp.create_dataset(density_name, data=density_data)\n",
        "\n",
        "            if not group_success and error_msg:\n",
        "                failed_groups.append((group_name, error_msg))\n",
        "            else:\n",
        "                successful_groups += 1\n",
        "\n",
        "    print(f\"Current density conversion: {successful_groups}/{total_groups} groups successful\")\n",
        "\n",
        "    return failed_groups"
      ],
      "metadata": {
        "id": "_PCPABf-7wMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "failed_groups = add_current_density_datasets(cell_h5_loc)\n",
        "print_h5_tree(cell_h5_loc,head=45)"
      ],
      "metadata": {
        "id": "cJrX2vdg4RQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def get_datasets_from_h5(h5_file, dataset_names, attributes=None):\n",
        "    \"\"\"\n",
        "    Extract datasets and attributes from HDF5 file as DataFrame\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    h5_file : str\n",
        "        Path to HDF5 file\n",
        "    dataset_names : list or str\n",
        "        Dataset name(s) to extract. Can be a single string or list of strings.\n",
        "    attributes : list or str, optional\n",
        "        Attribute(s) to include as columns. Can be a single string or list of strings.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame\n",
        "        DataFrame with columns: cell_id, attributes (if provided), and dataset_names\n",
        "    \"\"\"\n",
        "    # Ensure inputs are lists\n",
        "    if isinstance(dataset_names, str):\n",
        "        dataset_names = [dataset_names]\n",
        "    if isinstance(attributes, str):\n",
        "        attributes = [attributes]\n",
        "    elif attributes is None:\n",
        "        attributes = []\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    with h5py.File(h5_file, 'r') as hf:\n",
        "        for group_name in hf.keys():\n",
        "            group = hf[group_name]\n",
        "\n",
        "            row = {'cell_id': group.attrs.get('cell_id', group_name)}\n",
        "\n",
        "            # Extract requested attributes\n",
        "            for attr in attributes:\n",
        "                if attr in group.attrs:\n",
        "                    row[attr] = group.attrs[attr]\n",
        "                else:\n",
        "                    row[attr] = np.nan\n",
        "\n",
        "            # Extract requested datasets\n",
        "            for dataset_name in dataset_names:\n",
        "                if dataset_name in group:\n",
        "                    value = group[dataset_name][()]\n",
        "                    # Handle scalar arrays\n",
        "                    if isinstance(value, np.ndarray) and value.size == 1:\n",
        "                        value = value.item()\n",
        "                    row[dataset_name] = value\n",
        "                else:\n",
        "                    row[dataset_name] = np.nan\n",
        "\n",
        "            rows.append(row)\n",
        "    df = pd.DataFrame(rows)\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DZel16GTgRmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten_nested_column(df, group_col, nested_col):\n",
        "    \"\"\"\n",
        "    Flatten a column containing list of [key, value] pairs.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame with nested column\n",
        "    - group_col: column to group by (e.g., 'cell_id', 'genotype')\n",
        "    - nested_col: column with nested data like [[0, 5], [10, 12], ...]\n",
        "\n",
        "    Returns:\n",
        "    - dict of DataFrames, one per group\n",
        "    \"\"\"\n",
        "    result_dfs = {}\n",
        "\n",
        "    for group_name, group_df in df.groupby(group_col):\n",
        "        # Collect all the nested data for this group\n",
        "        rows_data = {}\n",
        "\n",
        "        for idx, row in group_df.iterrows():\n",
        "            cell_id = row.get('cell_id', idx)  # Or whatever identifier\n",
        "            nested_data = row[nested_col]\n",
        "\n",
        "            # Skip NaN/None entries\n",
        "            if nested_data is None:\n",
        "                continue\n",
        "\n",
        "            # Check if it's a scalar NaN (not a list)\n",
        "            if not isinstance(nested_data, (list, np.ndarray)):\n",
        "                if pd.isna(nested_data):\n",
        "                    continue\n",
        "\n",
        "            # Skip empty lists\n",
        "            if len(nested_data) == 0:\n",
        "                continue\n",
        "\n",
        "            # Convert [[key, val], [key, val]...] to dict\n",
        "            for key, val in nested_data:\n",
        "                if key not in rows_data:\n",
        "                    rows_data[key] = {}\n",
        "                rows_data[key][cell_id] = val\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        result_dfs[group_name] = pd.DataFrame(rows_data).T.sort_index()\n",
        "\n",
        "    return result_dfs"
      ],
      "metadata": {
        "id": "i3d_HcQR2M8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_dict_to_excel(result_dfs, filename):\n",
        "    \"\"\"\n",
        "    Write dictionary of DataFrames to Excel with each DataFrame as a separate sheet.\n",
        "\n",
        "    Parameters:\n",
        "    - result_dfs: dict where keys are sheet names, values are DataFrames\n",
        "    - filename: output Excel filename (e.g., 'results.xlsx')\n",
        "    \"\"\"\n",
        "    with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
        "        for sheet_name, df in result_dfs.items():\n",
        "            df.to_excel(writer, sheet_name=str(sheet_name))\n",
        "\n",
        "    print(f\"Wrote {len(result_dfs)} sheets to {filename}\")\n",
        "    print(os.getcwd())\n",
        "    return None"
      ],
      "metadata": {
        "id": "5cE6XPqf2KvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_key = 'density_IV_Na_0.2_10'\n",
        "\n",
        "# Multiple datasets with multiple attributes\n",
        "df = get_datasets_from_h5(cell_h5_loc,\n",
        "                          [data_key],\n",
        "                          ['genotype'])\n",
        "\n",
        "result_dfs  = flatten_nested_column(df,'genotype',data_key)\n",
        "\n",
        "for k,v in result_dfs.items():\n",
        "    print(k)\n",
        "\n",
        "write_dict_to_excel(result_dfs, data_key+'.xlsx')"
      ],
      "metadata": {
        "id": "tpyFIKSj1MvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-7J0bJ8fgRhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# def init_func_arg_dicts_h5():\n",
        "\n",
        "#     # Spike argument definitions\n",
        "#     spike_args_gain = {'spike_thresh': 10, 'high_dv_thresh': 20, 'low_dv_thresh': -5, 'window_ms': 3}\n",
        "#     spike_args_rheo = {'spike_thresh': 15, 'high_dv_thresh': 30, 'low_dv_thresh': -15, 'window_ms': 2}\n",
        "#     spike_args_late = {'spike_thresh': 10, 'high_dv_thresh': 30, 'low_dv_thresh': -5, 'window_ms': 3}\n",
        "\n",
        "#     analyzer_configs = {\n",
        "#         'VC - 3min GapFree': {\n",
        "#             'func': rmp_analyzer,\n",
        "#             'to_plot': True\n",
        "#         },\n",
        "\n",
        "#         'I0 - 3min GapFree': {\n",
        "#             'func': rmp_analyzer,\n",
        "#             'to_plot': True\n",
        "#         },\n",
        "\n",
        "#         'IC - Rheobase': {\n",
        "#             'func': rheobase_analyzer_V2,\n",
        "#             'spike_args': spike_args_rheo,\n",
        "#             'to_plot': True,\n",
        "#             'verbose': False,\n",
        "#             'single_spike': False\n",
        "#         },\n",
        "\n",
        "#         'IC - Gain - D10pA': {\n",
        "#             'func': gain_analyzer_v2,\n",
        "#             'spike_args': spike_args_gain,\n",
        "#             'to_plot': 1,\n",
        "#             'max_fit_steps': 4,\n",
        "#             'rel_slope_cut': 0.7,\n",
        "#             'Vh_hilo': [-60, -80]\n",
        "#         },\n",
        "\n",
        "#         'IC - Gain - D20pA': {\n",
        "#             'func': gain_analyzer_v2,\n",
        "#             'spike_args': spike_args_gain,\n",
        "#             'to_plot': 1,\n",
        "#             'max_fit_steps': 4,\n",
        "#             'rel_slope_cut': 0.7,\n",
        "#             'Vh_hilo': [-60, -80]\n",
        "#         },\n",
        "\n",
        "#         'IC - Gain - D25pA': {\n",
        "#             'func': gain_analyzer_v2,\n",
        "#             'spike_args': spike_args_gain,\n",
        "#             'to_plot': 1,\n",
        "#             'max_fit_steps': 4,\n",
        "#             'rel_slope_cut': 0.7,\n",
        "#             'Vh_hilo': [-60, -80]\n",
        "#         },\n",
        "\n",
        "#         'IC - Gain - D50pA': {\n",
        "#             'func': gain_analyzer_v2,\n",
        "#             'spike_args': spike_args_gain,\n",
        "#             'to_plot': 1,\n",
        "#             'max_fit_steps': 4,\n",
        "#             'rel_slope_cut': 0.7,\n",
        "#             'Vh_hilo': [-60, -80]\n",
        "#         },\n",
        "\n",
        "#         'VC - MemTest-10ms-160ms': {\n",
        "#             'func': membrane_analyzer,\n",
        "#             'to_plot': True,\n",
        "#             'verbose': False\n",
        "#         },\n",
        "\n",
        "#         'IC - Latentcy 800pA-1s': {\n",
        "#             'func': latencey_analyzer,\n",
        "#             'spike_args': spike_args_late,\n",
        "#             'to_plot': True\n",
        "#         },\n",
        "\n",
        "#         'IC - R input': {\n",
        "#             'func': input_resistance_analyzer,\n",
        "#             'dVm_limits': [-30, 10],\n",
        "#             'to_plot': True\n",
        "#         },\n",
        "\n",
        "#         'VC - Multi IV - 150ms': {\n",
        "#             'func': IV_analyzer_v4,\n",
        "#             'Na_window': [0.2, 10],\n",
        "#             'K_window': [130, 140],\n",
        "#             'to_plot': True,\n",
        "#             'leak_threshold': -400,\n",
        "#             'use_PN': True,\n",
        "#             'PN_voltages': [-60, -80, -90],\n",
        "#             'use_baseline_subtraction': True,\n",
        "#             'to_plot_PN': True\n",
        "#         },\n",
        "\n",
        "#         'VC - Multi IV - 500ms': {\n",
        "#             'func': IV_analyzer_v4,\n",
        "#             'Na_window': [0.2, 10],\n",
        "#             'K_window': [130, 140],\n",
        "#             'to_plot': True,\n",
        "#             'leak_threshold': -400,\n",
        "#             'use_PN': True,\n",
        "#             'PN_voltages': [-60, -80, -90],\n",
        "#             'use_baseline_subtraction': True,\n",
        "#             'to_plot_PN': True\n",
        "#         },\n",
        "\n",
        "#     }\n",
        "#     return analyzer_configs"
      ],
      "metadata": {
        "id": "WbGd5ec4gReV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}